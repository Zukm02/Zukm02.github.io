<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Animal Image Captioning Model</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 800px;
      margin: 2rem auto;
      padding: 1rem;
      line-height: 1.6;
    }
    img {
      max-width: 100%;
      border-radius: 10px;
      margin: 1rem 0;
    }
    iframe {
      width: 100%;
      height: 480px;
      border: none;
      margin: 2rem 0;
    }
    a {
      color: #0077cc;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <h1>Animal Image Captioning Model</h1>

  <p>This project combines computer vision and natural language processing to classify images of animals and generate descriptive captions. Using the COCO dataset and deep learning models, we built a two-stage system to identify and describe the contents of an image.</p>

  <h2>üì¶ Dataset</h2>
  <ul>
    <li>Started with the 2017 COCO dataset: 118,287 total images</li>
    <li>Filtered to 23,989 images containing animals</li>
    <li>Resized, normalized, and managed using the <code>FiftyOne</code> library</li>
  </ul>

  <h2>üîç Step 1: Image Classification with YOLOv8</h2>
  <p>We used both CPU and GPU variants of YOLOv8 to classify animal types. Early experiments showed overfitting when trained too long, so we reverted to a 20-epoch optimized model. YOLO allowed us to localize animals in the image and tag them for captioning.</p>

  <h2>üìù Step 2: Caption Generation with BLIP</h2>
  <p>We used the <code>BlipForConditionalGeneration</code> model and <code>BlipProcessor</code> to generate captions. BLIP was tested alone and also in tandem with YOLO output to improve caption quality. This vision-language model translated visual content into short descriptions.</p>

  <h2>üê∂ Demo: Captioning My Dog</h2>
  <p>We tested the final model on a real photo of my dog. YOLO identified the object as a dog, and BLIP generated the caption: <em>"a white dog laying on a blanket."</em> This shows how the system works end-to-end ‚Äî from visual detection to natural language output.</p>

  <h2>‚ñ∂Ô∏è Project Demo</h2>
  <iframe src="https://docs.google.com/file/d/1TohGa-BkTtgys8I2-ALaBveZXwS7GcUB/preview"></iframe>

  <h2>üìΩÔ∏è Presentation Slides</h2>
  <iframe src="../files/Image_Captioning_Slides.pdf"></iframe>


  <h2>‚ö†Ô∏è Limitations</h2>
  <ul>
    <li>Small dataset of animal-only images may limit generalizability</li>
    <li>BLIP struggles with complex scenes or occlusion such as birds in ress from a panoramic view.</li>
    <li>Overlap between COCO 2014 & 2017 may affect evaluation</li>
  </ul>

  <h2>üìà Future Work</h2>
  <ul>
    <li>Improve dataset balance and diversity</li>
    <li>Apply image augmentation and class-specific loss tuning</li>
    <li>Explore alternative captioning models with more fine-tuning</li>
  </ul>

  <p><a href="../index.html">‚Üê Back to Portfolio</a></p>

</body>
</html>

